{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04 Extraction Features Using Basic Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hl/6492kk1939x_mv_8fr9qfz_40000gn/T/ipykernel_35019/1278749470.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ta as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data_path = '../../data/row/'\n",
    "processed_data_path = '../../data/processed/'\n",
    "data = pd.read_csv(\n",
    "    processed_data_path + 'features_NVDA.csv', index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = data.drop(['close', '4. close'], axis=1)\n",
    "y = data['close']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression\n",
    "\n",
    "Train a Gradient Boosting Regression model within a cross-validation loop, compute RMSE for each split, and use PCA if necessary due to high dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 82.95039864817232\n"
     ]
    }
   ],
   "source": [
    "rmse_scores = []\n",
    "\n",
    "for test_train, test_index in tscv.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[test_train], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[test_train], y.iloc[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate RMSE and append to scores list\n",
    "    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "print(f\"Average RMSE: {np.mean(rmse_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance and Model Comparison\n",
    "\n",
    "After training, extract feature importance and consider removing low-importance features or testing other models for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "                 Feature  Importance\n",
      "8                   MA-5    0.890773\n",
      "2                    low    0.042797\n",
      "1                   high    0.023779\n",
      "15                 EMA12    0.011197\n",
      "0                   open    0.009313\n",
      "20           Bollinger_U    0.006534\n",
      "9                  MA-30    0.006205\n",
      "14                 SMA10    0.005310\n",
      "21           Bollinger_L    0.001665\n",
      "13               z_score    0.001000\n",
      "19           Bollinger_M    0.000680\n",
      "12           Williams_%R    0.000381\n",
      "7          daily_returns    0.000134\n",
      "17                   RoC    0.000109\n",
      "11        5-day_variance    0.000081\n",
      "22                 MOM12    0.000011\n",
      "18                   K15    0.000011\n",
      "10                   RSI    0.000007\n",
      "16                  MACD    0.000006\n",
      "3                 volume    0.000005\n",
      "6             log_volume    0.000003\n",
      "5   8. split coefficient    0.000000\n",
      "4     7. dividend amount    0.000000\n"
     ]
    }
   ],
   "source": [
    "feature_importance = model.feature_importances_\n",
    "# Filter or modify features based on importance as needed\n",
    "feature_names = X.columns\n",
    "\n",
    "# Combine feature names and their importances into a DataFrame for easier analysis\n",
    "importances_df = pd.DataFrame(\n",
    "    {'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Display the sorted importances\n",
    "print(\"Feature Importances:\")\n",
    "print(importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing the Dimensions with Set Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold below which features are considered of low importance\n",
    "# For demonstration, using 0.01 (1%) as the threshold\n",
    "threshold = 0.001\n",
    "\n",
    "# Identify features with importance below the threshold\n",
    "low_importance_features = importances_df[importances_df['Importance']\n",
    "                                         < threshold]['Feature']\n",
    "\n",
    "X_filtered1 = X.drop(columns=low_importance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train(X, y, model_func=GradientBoostingRegressor()):\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model = model_func\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate RMSE and append to scores list\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    print(f\"Average RMSE: {np.mean(rmse_scores)}\")\n",
    "    feature_importance = model.feature_importances_\n",
    "    # Filter or modify features based on importance as needed\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Combine feature names and their importances into a DataFrame for easier analysis\n",
    "    importances_df = pd.DataFrame(\n",
    "        {'Feature': feature_names, 'Importance': feature_importance})\n",
    "\n",
    "    # Sort the DataFrame by importance in descending order\n",
    "    importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "    # Display the sorted importances\n",
    "    print(\"Feature Importances:\")\n",
    "    print(importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 91.02298686390976\n",
      "Feature Importances:\n",
      "       Feature  Importance\n",
      "3         MA-5    0.893919\n",
      "2          low    0.044035\n",
      "1         high    0.023748\n",
      "6        EMA12    0.010929\n",
      "0         open    0.009332\n",
      "4        MA-30    0.006421\n",
      "7  Bollinger_U    0.006370\n",
      "5        SMA10    0.004512\n",
      "8  Bollinger_L    0.000734\n"
     ]
    }
   ],
   "source": [
    "test_train(X_filtered1, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Dimensionality Reduction\n",
    "\n",
    "If the feature dimension is too large, you might consider applying PCA before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  PC1       PC2       PC3       PC4       PC5       PC6  \\\n",
      "date                                                                      \n",
      "2024-01-30  11.768584  3.795726  0.339991 -3.948547 -0.729412  0.065153   \n",
      "2024-01-29  10.779546  4.329662  1.270286 -2.579493 -0.203480  0.054095   \n",
      "2024-01-26  10.450967  4.519557  2.050041 -1.402683 -0.044201 -0.006534   \n",
      "2024-01-25  10.279822  3.753403  0.368083 -3.313611  0.093792  0.195549   \n",
      "2024-01-24  10.308375  3.704569  1.022974 -2.402368  0.040738  0.108888   \n",
      "\n",
      "                 PC7       PC8       PC9  \n",
      "date                                      \n",
      "2024-01-30  6.303305  0.507095  0.546658  \n",
      "2024-01-29  0.977580 -1.158515 -0.862616  \n",
      "2024-01-26 -0.897449 -1.725984 -1.395414  \n",
      "2024-01-25 -1.178938 -1.274401 -0.291720  \n",
      "2024-01-24 -1.138463 -1.346010 -0.594912  \n"
     ]
    }
   ],
   "source": [
    "# Apply PCA\n",
    "# Retain 95% of variance or choose the number of components\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Transform PCA results into a DataFrame with meaningful column names\n",
    "pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "# Ensure to match the original index\n",
    "X_pca_df = pd.DataFrame(X_pca, columns=pca_columns, index=X.index)\n",
    "\n",
    "# Now X_pca_df can be used for further analysis or modeling\n",
    "print(X_pca_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 49.18003227590566\n",
      "Feature Importances:\n",
      "  Feature  Importance\n",
      "0     PC1    0.950407\n",
      "7     PC8    0.019602\n",
      "2     PC3    0.018132\n",
      "1     PC2    0.007926\n",
      "6     PC7    0.002335\n",
      "3     PC4    0.000964\n",
      "8     PC9    0.000290\n",
      "4     PC5    0.000175\n",
      "5     PC6    0.000170\n"
     ]
    }
   ],
   "source": [
    "test_train(X=X_pca_df, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encapsulating the Process\n",
    "Finally, encapsulate your data preprocessing, model training, and prediction steps into a function for reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    data = data.rename(columns={\"1. open\": \"open\",\n",
    "                                \"2. high\": \"high\",\n",
    "                                \"3. low\": \"low\",\n",
    "                                \"5. adjusted close\": \"close\",\n",
    "                                \"6. volume\": \"volume\"})\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    data['log_volume'] = np.log(data['volume'])\n",
    "\n",
    "    data['daily_returns'] = data['close'].diff()\n",
    "\n",
    "    data['MA-5'] = data['close'].rolling(window=5).mean()\n",
    "\n",
    "    data['MA-30'] = data['close'].rolling(window=30).mean()\n",
    "\n",
    "    RSI = ta.momentum.RSIIndicator(data['close'], window=14)\n",
    "    data['RSI'] = RSI.rsi()\n",
    "\n",
    "    data['5-day_variance'] = data['close'].rolling(window=5).var()\n",
    "\n",
    "    WILLR = ta.momentum.WilliamsRIndicator(high=data['high'],\n",
    "                                           low=data['low'],\n",
    "                                           close=data['close'],\n",
    "                                           lbp=14)\n",
    "    data['Williams_%R'] = WILLR.williams_r()\n",
    "\n",
    "    data['z_score'] = (data['close'] - data['close'].rolling(window=10).mean()\n",
    "                       ) / data['close'].rolling(window=10).std()\n",
    "\n",
    "    data['SMA10'] = data['close'].rolling(window=10).mean()\n",
    "\n",
    "    data['EMA12'] = data['close'].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    MACD = ta.trend.MACD(close=data['close'], window_fast=12, window_slow=26)\n",
    "    data['MACD'] = MACD.macd()\n",
    "\n",
    "    RoC = ta.momentum.ROCIndicator(close=data['close'], window=1)\n",
    "    data['RoC'] = RoC.roc()\n",
    "\n",
    "    low_min = data['low'].rolling(window=15).min()\n",
    "    high_max = data['high'].rolling(window=15).max()\n",
    "    data['K15'] = ((data['close'] - low_min) /\n",
    "                   (high_max - low_min)) * 100\n",
    "\n",
    "    data['Bollinger_M'] = data['close'].rolling(window=20).mean()\n",
    "    data['Bollinger_U'] = data['Bollinger_M'] + \\\n",
    "        2 * data['close'].rolling(window=20).std()\n",
    "    data['Bollinger_L'] = data['Bollinger_M'] - \\\n",
    "        2 * data['close'].rolling(window=20).std()\n",
    "\n",
    "    data['MOM12'] = data['close'] - data['close'].shift(12)\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "def pca_reduction(data):\n",
    "    # Define features and target variable\n",
    "    X = data.drop(['close', '4. close'], axis=1)\n",
    "    y = data['close']\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Retain 95% of variance or choose the number of components\n",
    "    pca = PCA(n_components=0.95)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    return X_pca, y\n",
    "    \n",
    "def train_and_predict(X_pca, y, model_func=GradientBoostingRegressor()):    \n",
    "    X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "    # Initialize TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    rmse_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Train the model\n",
    "        model = model_func\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate RMSE and append to scores list\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    print(f\"Average RMSE: {np.mean(rmse_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def financial_forecast_pipeline(data, model_func=GradientBoostingRegressor()):\n",
    "    data_more_features = add_features(data)\n",
    "    X_pca, y = pca_reduction(data=data_more_features)\n",
    "    train_and_predict(X_pca=X_pca, y=y, model_func=model_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVDA_5_years_daily_data = pd.read_csv(\n",
    "    row_data_path + 'NVDA_5_years_daily_data.csv', index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 49.198931642728176\n"
     ]
    }
   ],
   "source": [
    "financial_forecast_pipeline(NVDA_5_years_daily_data,\n",
    "                            model_func=GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 62.90965647670862\n"
     ]
    }
   ],
   "source": [
    "financial_forecast_pipeline(NVDA_5_years_daily_data,\n",
    "                            model_func=LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE: 48.371460059514895\n"
     ]
    }
   ],
   "source": [
    "financial_forecast_pipeline(NVDA_5_years_daily_data,\n",
    "                            model_func=tree.DecisionTreeRegressor())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
